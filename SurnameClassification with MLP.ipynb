{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SurnameClassification with MLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf2pFVS0U1mQ"
      },
      "source": [
        "**Initialization**\n",
        "* I use these 3 lines of code on top of my each Notebooks because it will help to prevent any problems while reloading and reworking on a same Project or Problem. And the third line of code helps to make visualization within the Notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJEtOByNa5Vo"
      },
      "source": [
        "#@ Initialization:\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-68Qtx0PVX9z"
      },
      "source": [
        "**Downloading the Libraries and Dependencies**\n",
        "* I have downloaded all the Libraries and Dependencies required for this Project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJYhZ4poVWSw"
      },
      "source": [
        "#@ Downloading the Libraries and Dependencies:\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import collections\n",
        "import re, json, string\n",
        "\n",
        "from argparse import Namespace\n",
        "from IPython.display import display\n",
        "from collections import Counter\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm_notebook"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_L1e2rEXGfz"
      },
      "source": [
        "**Getting the Data**\n",
        "* I have used Google Colab for this Project so the process of downloading and reading the Data might be different in other platforms. I have used **The Surname Dataset** which is a collection of 10000 surnames from 18 different Nationalities collected from different name sources on the Internet. The first property of this Dataset is that it is fairly Imbalanced. The second property is that there is a valid and intuitive relationships between Nationality origin and Surname Orthography. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9DpbXAyXDhy",
        "outputId": "0f2ac5e0-6f3f-4e1d-91c7-a205df3e32e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        }
      },
      "source": [
        "#@ Getting the Dataset:\n",
        "args = Namespace(\n",
        "    raw_dataset = \"/content/drive/My Drive/Colab Notebooks/Surname/surnames.csv\",\n",
        "    train_proportion = 0.7,\n",
        "    val_proportion = 0.15,\n",
        "    test_proportion = 0.15,\n",
        "    output_munged = \"/content/drive/My Drive/Colab Notebooks/Surname/surnames_with_splits.csv\",\n",
        "    seed = 42\n",
        ")\n",
        "\n",
        "#@ Reading the Raw Dataset:\n",
        "surnames = pd.read_csv(args.raw_dataset, header=0)\n",
        "display(surnames.head(10))                                                                          # Inspecting the DataFrame.\n",
        "print(\"\\nUnique Classes:\")\n",
        "display(set(surnames[\"nationality\"]))                                                               # Inspecting the Unique classes in the Dataset."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>surname</th>\n",
              "      <th>nationality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Woodford</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Coté</td>\n",
              "      <td>French</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Kore</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Koury</td>\n",
              "      <td>Arabic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Lebzak</td>\n",
              "      <td>Russian</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Obinata</td>\n",
              "      <td>Japanese</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Rahal</td>\n",
              "      <td>Arabic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Zhuan</td>\n",
              "      <td>Chinese</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Acconci</td>\n",
              "      <td>Italian</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Mifsud</td>\n",
              "      <td>Arabic</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    surname nationality\n",
              "0  Woodford     English\n",
              "1      Coté      French\n",
              "2      Kore     English\n",
              "3     Koury      Arabic\n",
              "4    Lebzak     Russian\n",
              "5   Obinata    Japanese\n",
              "6     Rahal      Arabic\n",
              "7     Zhuan     Chinese\n",
              "8   Acconci     Italian\n",
              "9    Mifsud      Arabic"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Unique Classes:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'Arabic',\n",
              " 'Chinese',\n",
              " 'Czech',\n",
              " 'Dutch',\n",
              " 'English',\n",
              " 'French',\n",
              " 'German',\n",
              " 'Greek',\n",
              " 'Irish',\n",
              " 'Italian',\n",
              " 'Japanese',\n",
              " 'Korean',\n",
              " 'Polish',\n",
              " 'Portuguese',\n",
              " 'Russian',\n",
              " 'Scottish',\n",
              " 'Spanish',\n",
              " 'Vietnamese'}"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kQ9dAp-ce-f"
      },
      "source": [
        "**Processing the Dataset** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2-I7utNbpz1",
        "outputId": "6460c3dd-c8da-4b9e-a778-3b7cf12597f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "#@ Splitting the Dataset on the basis of Nationality:\n",
        "by_nationality = collections.defaultdict(list)                      # Collection stores the collection of Data.\n",
        "for _, row in surnames.iterrows():\n",
        "  by_nationality[row.nationality].append(row.to_dict())             # Creating the Dictionary.\n",
        "\n",
        "#@ Creating the Split Data:\n",
        "final_list = []\n",
        "np.random.seed(args.seed)\n",
        "for _, item_list in sorted(by_nationality.items()):\n",
        "  np.random.shuffle(item_list)                                      # Shuffling the Data randomly.\n",
        "  n = len(item_list)                                                # Number of Items.\n",
        "  n_train = int(args.train_proportion * n)                          # Number of Training Dataset.\n",
        "  n_val = int(args.val_proportion * n)                              # Number of Validation Dataset.\n",
        "  n_test = int(args.test_proportion * n)                            # Number of Testing Dataset.\n",
        "  #@ Giving the Data point a Split Attribute:\n",
        "  for item in item_list[:n_train]:\n",
        "    item[\"split\"] = \"train\"                                         # Training Dataset.\n",
        "  for item in item_list[n_train:n_train+n_val]:\n",
        "    item[\"split\"] = \"val\"                                           # Validation Dataset.\n",
        "  for item in item_list[n_train+n_val:n_train+n_val+n_test]:\n",
        "    item[\"split\"] = \"test\"                                          # Testing Dataset.\n",
        "  #@ Adding to the Final List:\n",
        "  final_list.extend(item_list)\n",
        "\n",
        "#@ Final Split of the Data and Creating the Final DataFrame:\n",
        "final_surnames = pd.DataFrame(final_list)\n",
        "\n",
        "#@ Inspecting the Final DataFrame:\n",
        "display(final_surnames.split.value_counts())                         # Inspecting the Training, Validation and the Testing Data.\n",
        "print(\" \")\n",
        "display(final_surnames.head())                                       # Inspecting the Final DataFrame."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train    7680\n",
              "test     1640\n",
              "val      1640\n",
              "Name: split, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>surname</th>\n",
              "      <th>nationality</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Guirguis</td>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Shamon</td>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Nader</td>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Kassis</td>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bahar</td>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    surname nationality  split\n",
              "0  Guirguis      Arabic  train\n",
              "1    Shamon      Arabic  train\n",
              "2     Nader      Arabic  train\n",
              "3    Kassis      Arabic  train\n",
              "4     Bahar      Arabic  train"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVMlfnQyiS8G"
      },
      "source": [
        "#@ Preparing the Final Data:\n",
        "final_surnames.to_csv(args.output_munged, index=False)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ2CTZq1agOO"
      },
      "source": [
        "**Surname Dataset Class**\n",
        "* PyTorch provides an abstraction for the Dataset by providing a Dataset Class. The Dataset Class is an abstract Operator. When using PyTorch with a new Dataset it is necessary to sub class the Dataset Class and Implement the getitem and len methods. I will implement two functions: the getitem method which returns a Data point when given an index and len method returns the length of the Dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxtzyCZTXh-H"
      },
      "source": [
        "#@ Implementing the Surname Dataset Class:\n",
        "class SurnameDataset(Dataset):\n",
        "  def __init__(self, surname_df, vectorizer):\n",
        "    \"\"\"\n",
        "    Args: surname_df(pandas DataFrame): The Dataset.\n",
        "        : vectorizer(SurnameVectorizer): Vectorizer Instantiated.\n",
        "    \"\"\"\n",
        "    self.surname_df = surname_df\n",
        "    self._vectorizer = vectorizer \n",
        "\n",
        "    self.train_df = self.surname_df[self.surname_df.split == \"train\"]\n",
        "    self.train_size = len(self.train_df)\n",
        "\n",
        "    self.val_df = self.surname_df[self.surname_df.split == \"val\"]\n",
        "    self.validation_size = len(self.val_df)\n",
        "\n",
        "    self.test_df = self.surname_df[self.surname_df.split == \"test\"]\n",
        "    self.test_size = len(self.test_df)\n",
        "\n",
        "    self._lookup_dict = {\"train\": (self.train_df, self.train_size),\n",
        "                         \"val\": (self.val_df, self.validation_size),\n",
        "                         \"test\": (self.test_df, self.test_size)}\n",
        "    \n",
        "    self.set_split(\"train\")\n",
        "\n",
        "    #@ Dataset Class Weights:\n",
        "    class_counts = surname_df.nationality.value_counts().to_dict()\n",
        "    def sort_key(item):\n",
        "      return self._vectorizer.nationality_vocab.lookup_token(item[0])\n",
        "    sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
        "    frequencies = [count for _, count in sorted_counts]\n",
        "    self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
        "\n",
        "  @classmethod\n",
        "  def load_dataset_and_make_vectorizer(cls, surname_csv):\n",
        "    \"\"\"Load Dataset and make Vectorizer from scratch.\n",
        "    Args: surname_csv: Location of the Dataset.\n",
        "    Returns: An instance of the SurnameDataset.\n",
        "    \"\"\"\n",
        "    surname_df = pd.read_csv(surname_csv)\n",
        "    train_surname_df = surname_df[surname_df.split == \"train\"]\n",
        "    return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df))\n",
        "  \n",
        "  @classmethod\n",
        "  def load_dataset_and_load_vectorizer(cls, surname_csv, vectorizer_filepath):\n",
        "    \"\"\"Load Dataset and corresponding Vectorizer.\n",
        "    Args: surname_csv: Location of the Dataset.\n",
        "        : vectorizer_filepath: Location of the saved Vectorizer.\n",
        "    Returns: An instance of the SurnameDataset.\n",
        "    \"\"\"\n",
        "    surname_df = pd.read_csv(surname_csv)\n",
        "    vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
        "    return cls(surname_df, vectorizer)\n",
        "  \n",
        "  @staticmethod\n",
        "  def load_vectorizer_only(vectorizer_filepath):\n",
        "    \"\"\"A static method for loading the Vectorizer from file.\n",
        "    Args: vectorizer_filepath: The location of the serialized vectorizer.\n",
        "    Returns: An instance of SurnameVectorizer.\n",
        "    \"\"\"\n",
        "    with open(vectorizer_filepath) as fp:\n",
        "      return SurnameVectorizer.from_serializable(json.load(fp))\n",
        "  \n",
        "  def save_vectorizer(self, vectorizer_filepath):\n",
        "    \"\"\"Saves the Vectorizer to disk using json.\n",
        "    Args: vectorizer_filepath: The location to save the Vectorizer.\n",
        "    \"\"\"\n",
        "    with open(vectorizer_filepath, \"w\") as fp:\n",
        "      json.dump(self._vectorizer.to_serializable(), fp)\n",
        "  \n",
        "  def get_vectorizer(self):\n",
        "    return self._vectorizer\n",
        "  \n",
        "  def set_split(self, split=\"train\"):\n",
        "    self._target_split(split)\n",
        "    self._target_df, self._target_size = self._lookup_dict[split]\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self._target_size\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    row = self._target_df.iloc[index]\n",
        "    surname_vector = self._vectorizer.vectorize(row.surname)\n",
        "    nationality_index = self._vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
        "    return {\"x_surname\": surname_vector,\n",
        "            \"y_nationality\": nationality_index}\n",
        "  \n",
        "  def get_num_batches(self, batch_size):\n",
        "    return len(self) // batch_size\n",
        "  \n",
        "def generate_batches(dataset, batch_size, shuffle=True,\n",
        "                       drop_last=True, device=\"cpu\"):\n",
        "  dataloader = DataLoader(dataset=dataset, batch_size=batch_size, \n",
        "                          shuffle=shuffle, drop_last=drop_last)\n",
        "  for data_dict in dataloader:\n",
        "    out_data_dict = {}\n",
        "    for name, tensor in data_dict.items():\n",
        "      out_data_dict[name] = data_dict[name].to(device)\n",
        "    yield out_data_dict"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rov8zBSbND8Y"
      },
      "source": [
        "**The Vocabulary Class**\n",
        "* The Vocabulary is the coordination of two Python Dictionaries that form a bijection between tokens or characters here and integers. The first dictionary maps characters to integers indices and the second maps the integers indices to characters. The add_token method is used to add new tokens into the Vocabulary and look_up method is used to retrieve an index and lookup_index is used to retrieve a token given an index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyvE8cT2GQ_M"
      },
      "source": [
        "#@ The Vocabulary Class:\n",
        "class Vocabulary(object):\n",
        "  \"\"\" Class to process text and extract Vocabulary for mapping. \"\"\"\n",
        "  def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
        "    \"\"\"\n",
        "    Args: token_to_idx(dict): Pre existing map of Tokens to Index.\n",
        "        : add_unk(bool): A flag indicating whether to add UNK Token.\n",
        "        : unk_token(string): The UNK Token to add in Vocabulary.\n",
        "    \"\"\"\n",
        "    if token_to_idx is None:\n",
        "      token_to_idx = {}\n",
        "    self._token_to_idx = token_to_idx\n",
        "    self._idx_to_token = {idx:token for token, idx in self._token_to_idx.items()}\n",
        "    self._add_unk = add_unk\n",
        "    self._unk_token = unk_token\n",
        "\n",
        "    self.unk_index = -1\n",
        "    if add_unk:\n",
        "      self.unk_index = self.add_token(unk_token)\n",
        "    \n",
        "  def to_serializable(self):\n",
        "    \"\"\"Returns a dictionary that can be serialized.\n",
        "    \"\"\"\n",
        "    return {\"token_to_idx\":self._token_to_idx,\n",
        "            \"add_unk\":self._add_unk,\n",
        "            \"unk_token\":self._unk_token}\n",
        "  \n",
        "  @classmethod\n",
        "  def from_serializable(cls, contents):\n",
        "    return cls(**contents)\n",
        "  \n",
        "  def add_token(self, token):\n",
        "    \"\"\"Update the mapping dictionary based on the Tokens.\n",
        "    Args: token: The item to add into the Vocabulary.\n",
        "    Returns: index: Integer corresponding to the Token.\n",
        "    \"\"\"\n",
        "    try:\n",
        "      index = self._token_to_idx[token]\n",
        "    except KeyError:\n",
        "      index = len(self._token_to_idx)\n",
        "      self._token_to_idx[token] = index\n",
        "      self._idx_to_token[index] = token\n",
        "    return index\n",
        "  \n",
        "  def add_many(self, tokens):\n",
        "    \"\"\"Add a list of Tokens into Vocabulary.\n",
        "    Args: tokens(list): A list of string Tokens.\n",
        "    Returns: indices(list): A list of indices correspoinding to the Tokens.\n",
        "    \"\"\"\n",
        "    return [self.add_token(token) for token in tokens]\n",
        "  \n",
        "  def lookup_token(self, token):\n",
        "    \"\"\"Retrieve the Index associated with the Token.\n",
        "    Args: token(str): The Token to lookup.\n",
        "    Returns: index(int): The Index correspoinding to the Token.\n",
        "    \"\"\"\n",
        "    if self.unk_index >= 0:\n",
        "      return self._token_to_idx.get(token, self.unk_index)\n",
        "    else:\n",
        "      return self._token_to_idx[token]\n",
        "  \n",
        "  def lookup_index(self, index):\n",
        "    \"\"\"Return the Token associated with the Index.\n",
        "    Args: index(int): The Index to lookup.\n",
        "    Returns: token(str): The Token correspoinding to the Index.\n",
        "    \"\"\"\n",
        "    if index not in self._idx_to_token:\n",
        "      raise KeyError(\"the index (%d) is not in the vocabulary\" % index)\n",
        "    return self._idx_to_token[index]\n",
        "  \n",
        "  def __str__(self):\n",
        "    return \"<Vocabulary(size=%d)>\" % len(self)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self._token_to_idx)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIORpomMXqYs"
      },
      "source": [
        "**The Vectorizer Class**\n",
        "* The Vocabulary converts individual tokens into Integers and The Surname Vectorizer is responsible for applying the Vocabulary and converting surname into Vector. Surnames are sequence of characters and each character is an individual token in the Vocabulary. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FVDRfwzWMBn"
      },
      "source": [
        "#@ The Vectorizer Class:\n",
        "class SurnameVectorizer(object):\n",
        "  \"\"\"The Vectorizer coordinates the Vocabularies and puts them to use.\n",
        "  \"\"\"\n",
        "  def __init__(self, surname_vocab, nationality_vocab):\n",
        "    self.surname_vocab = surname_vocab\n",
        "    self.nationality_vocab = nationality_vocab\n",
        "  \n",
        "  def vectorize(self, surname):\n",
        "    \"\"\"\n",
        "    Args: surname: The Surname\n",
        "    Returns: A collapsed one hot Encoding.\n",
        "    \"\"\"\n",
        "    vocab = self.surname_vocab\n",
        "    one_hot = np.zeros(len(vocab), dtype=np.float32)\n",
        "    for token in surname:\n",
        "      one_hot[vocab.lookup_token(token)] = 1\n",
        "    return one_hot\n",
        "  \n",
        "  @classmethod\n",
        "  def from_dataframe(cls, surname_df):\n",
        "    surname_vocab = Vocabulary(unk_token=\"@\")\n",
        "    nationality_vocab = Vocabulary(add_unk=False)\n",
        "\n",
        "    for index, row in surname_df.iterrows():\n",
        "      for letter in row.surname:\n",
        "        surname_vocab.add_token(letter)\n",
        "      nationality_vocab.add_token(row.nationality)\n",
        "\n",
        "    return cls(surname_vocab, nationality_vocab)\n",
        "  \n",
        "  @classmethod\n",
        "  def from_serializable(cls, contents):\n",
        "    surname_vocab = Vocabulary.from_serializable(contents['surname_vocab'])\n",
        "    nationality_vocab =  Vocabulary.from_serializable(contents['nationality_vocab'])\n",
        "    return cls(surname_vocab=surname_vocab, nationality_vocab=nationality_vocab)\n",
        "\n",
        "  def to_serializable(self):\n",
        "    return {'surname_vocab': self.surname_vocab.to_serializable(),\n",
        "            'nationality_vocab': self.nationality_vocab.to_serializable()}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFmJeafH0wHb"
      },
      "source": [
        "**The Model: Surname Classifier**\n",
        "* The Surname Classifier is an Implementation of the Multi Layer Perceptron. The first Linear Layer maps the input vectors to an intermediate vector and the non linearity is applied to that vector. A second Linear Layer maps the Intermediate vector to the Prediction vector. In the last step the Softmax Function is optionally applied to make sure the outputs sum to 1 which is interpreted as Probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybHQYe0BZZas"
      },
      "source": [
        "#@ The Surname Classifier using an MLP:\n",
        "class SurnameClassifier(nn.Module):\n",
        "  \"\"\" A Multi Layer Perceptron for classifying Surnames. \"\"\"\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    \"\"\"\n",
        "    Args: input_dim(int): The size of the Input Vectors.\n",
        "        : hidden_dim(int): The output size of the First Linear Layer.\n",
        "        : output_dim(int): The output size of the Second Lienar Layer.\n",
        "    \"\"\"\n",
        "    super(SurnameClassifier, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "    self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "  \n",
        "  def forward(self, x_in, apply_softmax=False):\n",
        "    \"\"\" The Forward pass of the Classifier. \"\"\"\n",
        "    intermediate_vector = F.relu(self.fc1(x_in))\n",
        "    prediction_vector = self.fc2(intermediate_vector)\n",
        "\n",
        "    if apply_softmax:\n",
        "      prediction_vector = F.softmax(prediction_vector, dim=1)\n",
        "    return prediction_vector"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iTcqiVVADRY"
      },
      "source": [
        "**The Training Routine**\n",
        "* The Training Routine is responsible for instantiating the Model, iterating over the Dataset, computing the output of the Model when the given data as Input, computing the Loss and updating the Model proportional to the Loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMCIkxfY_cHb"
      },
      "source": [
        "#@ Helper Functions for Training Routine:\n",
        "def make_train_state(args):\n",
        "  return {\"stop_early\": False,\n",
        "          \"early_stopping_step\": 0,\n",
        "          \"early_stopping_best_val\": 1e8,\n",
        "          \"learning_rate\": args.learning_rate,\n",
        "          \"epoch_index\": 0,\n",
        "          \"train_loss\": [],\n",
        "          \"train_acc\": [],\n",
        "          \"val_loss\": [],\n",
        "          \"val_acc\": [],\n",
        "          \"test_loss\": -1,\n",
        "          \"test_acc\": -1,\n",
        "          \"model_filename\": args.model_state_file}\n",
        "\n",
        "def update_train_stage(args, model, train_state):\n",
        "  \"\"\" Handles the Training state Updates. \"\"\"\n",
        "  #@ Saving atleast one Model:\n",
        "  if train_state[\"epoch_index\"] == 0:\n",
        "    torch.save(model.state_dict(), train_state[\"model_filename\"])\n",
        "    train_state[\"stop_early\"] = False\n",
        "  #@ Saving the Model if performance is improved:\n",
        "  elif train_state[\"epoch_index\"] >= 1:\n",
        "    loss_tm1, loss_t = train_state[\"val_loss\"][-2:]\n",
        "    #@ If the loss is worsened:\n",
        "    if loss_t >= train_state[\"early_stopping_best_val\"]:\n",
        "      train_state[\"early_stopping_step\"] += 1\n",
        "    else:\n",
        "      if loss_t < train_state[\"early_stopping_best_val\"]:\n",
        "        torch.save(model.state_dict(), train_state[\"model_filename\"])\n",
        "      train_state[\"early_stopping_step\"] = 0\n",
        "    train_state[\"stop_early\"] = train_state[\"early_stopping_step\"] >= args.early_stopping_criteria\n",
        "  return train_state\n",
        "\n",
        "def compute_accuracy(y_pred, y_target):\n",
        "  _, y_pred_indices = y_pred.max(dim=1)\n",
        "  n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
        "  return n_correct / len(y_pred_indices) * 100"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W78uTsLxJ4gh"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}